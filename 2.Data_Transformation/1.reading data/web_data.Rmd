---
title: "R Notebook"
output: html_notebook
---

# Reading from the Web

## Webscraping

* Programmatically extracting data from the HTML code of websites
* in some cases it can be illegal
* your IP can be blocked if you read too many pages too quickly

## Getting data off webpages - readlines()

```{r}
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")

htmlCode =readLines(con); close(con);
```

## Getting data off webpages - readlines()
```{r}
urlsample <- "http://biostat.jhsph.edu/~jleek/contact.html"

con_start = url(urlsample)

webdata = readLines(con_start)

close(con_start)
```

How many characters are in the 10th, 20th, 30th and 100th lines of webdata

```{r}
xxx <- webdata[c(10, 20, 30, 100)]

qqq <- nchar(xxx)

qqq
```

### read fixed width data from web
```{r}
fixwidths = c(1, 9, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4,4)

fixdata <- read.fwf(file = "./sample_data/getdata_wksst8110.for", 
                    widths = fixwidths, 
                    skip = 4)
fixdata[, c(1, 3, 6, 9, 12) ] <- NULL

answer <- sum(fixdata$V7)
answer
```

```{r}
fixed_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"

download.file(url = fixed_url, destfile = "./sample_data/fixed_data.for")

fixdata2 <- read.fwf(file = fixed_url, widths = fixwidths,
                     skip = 4, header = F)
fixdata2[, c(1, 3, 6, 9, 12) ] <- NULL
```



### Parsing With XML
```{r}
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"

# html <- htmlTreeParse(url, useInternalNodes = T)
```

```{r}
# xpathSApply(html, "//title", xmlValue)
```
```{r}
# xpathSapply(html, "//td[@id='col-citedby']", xmlValue)
```

### GET from the httr package
```{r}
library(httr); 

html2 = GET(url)

# extract as text
content2 = content(html2, as = "text")

parsedHtml = htmlParse(content2, asText = TRUE)
```


```{r}
xpathSApply(parsedHtml, "//title", xmlValue)
```


### Accessing websites with passwords
```{r}
library(httr)
pg1 = GET("http://httpbin.org/basic-auth/user/passwd")

pg1
```

```{r}
pg2 <- GET("http://httpbin.org/basic-auth/user/passwd",
           authenticate("user", "passwd"))

pg2
```
```{r}
names(pg2)
```

### Using Handles

Reading from the Web
```{r}
google = handle("http://google.com")

pg1 = GET(handle = google, path = "/")

pg2 = GET(handle = google, path = "search" )
```

httr package and R Bloggers has lots of web scraping examples

### Reading from APIs

Application programming interfaces

https://dev.twitter.com/apps

* create an dev account

* Create an application

* copy those key in your account and application


```{r}
library(httr)
myBearerToken <- "AAAAAAAAAAAAAAAAAAAAAKRrQQEAAAAAhyraogFBV1Ls%2F%2B5C4Nd623x4%2BN0%3DDNPfk6DuP2bmv7Dlk4lxhup0bxAhEwe4yL7xI3t3gWnYJrrVEP"
myApiKey <- "bUdFN8qROwLAAlm3GXDhMe4PG"
myApiSecretKey <- "13ebnhgRe9aVBVnSO8FzLaANhLTGkYXM8ahCWCnLPWb85VX43g"
myAccessToken <- "784096391526227968-ad3EWLw4aMOg7suO0K7lC4ouaD3RMuH"
myAccessTokenSecret <- "aODk3Rv9MBkCAitsUvSWpKwhcQ0Rk051JVJCiMTgvqgze"
```

```{r}
Sys.setenv(BEARER_TOKEN = myBearerToken)
```

```{r}
myapp = oauth_app("twitter", key = myApiKey,
                  secret = myApiSecretKey)

sig = sign_oauth1.0(myapp, token = myAccessToken,
                    token_secret = myAccessTokenSecret)

# specific URL: 1.1 is version; statuses is the theme of data;
# and data name; json is the only data type

sampleURL <- "https://api.twitter.com/2/tweets/search/stream"

homeTL = GET(sampleURL, sig)
```

## Converting the JSON object

```{r}
library(jsonlite)
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[]
```


## INTERACTING MORE DIRECTLY WITH FILES
```
file        : open a connection to a text  file
url         : open a connection to url
gzfile      : open a connection to .gz file
?connections


remember to close a connection
```

## FOREIGN PACKAGE

read.dta  (STATA)
read.mtp  (MINITAP)
read.octave
read.spss
read.xport (SAS)

You can read:, JPEG, PNG, MP3, GIS data

tuneR, seewave....























