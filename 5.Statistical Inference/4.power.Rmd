---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---


### Power

We've talked about a Type I error, rejecting the null hypothesis when it's true.  We've structured our hypothesis test so that the probability of this happening is small. The other kind of error we could make is to fail to reject when the alternative is true (Type II error). Or we might think about the probability of rejecting the null when it is false. This is called Power = 1 - Type II error. We don't have as much control over this probability, since we've spent all of our flexibility guaranteeing that the Type I error rate is small. 

One avenue for the control of power is at the design phase. There, assuming our finances let us, we can pick a large enough sample size so that we'd be likely to reject if the alternative is true. Thus the most frequent use of power is to help us design studies.

we'll discuss POWER, which is the probability of rejecting the null hypothesis when it is false, which is good and proper.

Power comes into play when you're designing an experiment, and in particular, if you're trying to determine if a null result (failing to reject a null hypothesis) is meaningful. For instance, you might have to determine if your sample size was big enough to yield a meaningful, rather than random, result.

Power gives you the opportunity to detect if your ALTERNATIVE hypothesis is true.

Beta is the probability of a Type II error, accepting a false null hypothesis; the complement of this is obviously (1 - beta) which represents the probability of rejecting a false null hypothesis. This is good and this is POWER!

Beta is the probability of a Type II error, accepting a false null hypothesis; the complement of this is obviously (1 - beta) which represents the probability of rejecting a false null hypothesis. This is good and this is POWER!


Suppose we're testing a null hypothesis H_0 with an alpha level of .05. Since H_a proposes that mu > 30 (the mean hypothesized by H_0), power is the probability that the true mean mu is greater than the (1-alpha) quantile or qnorm(.95). For simplicity, assume we're working with normal distributions of which we know the variances.

As you know, the shaded portion represents 5% of the area under the curve. If a test statistic fell in this shaded portion we would reject H_0 because the sample mean is too far from the mean (center) of the distribution hypothesized by H_0. Instead we would favor H_a, that mu > 30. This happens with probability .05.

First we have to emphasize a key point. The two hypotheses, H_0 and H_a, actually represent two distributions since they're talking about means or centers of distributions. H_0 says that the mean is mu_0 (30 in our example) and H_a says that the mean is mu_a.

We're assuming normality and equal variance, say sigma^2/n, for both hypotheses, so under H_0, X'~ N(mu_0, sigma^2/n) and under H_a, X'~ N(mu_a, sigma^2/n).

Here's a picture with the two distributions. We've drawn a vertical line at our favorite spot, at the 95th percentile of the red distribution. To the right of the line lies 5% of the red distribution.

Note that the placement of the vertical line depends on the null distribution. Here's another picture with fatter distributions. The vertical line is still at the 95th percentile of the null (red) distribution and 5% of the distribution still lies to its right. The line is calibrated to mu_0 and the variance.


We've shamelessly stolen plotting code from the slides so you can see H_a in action. Let's look at pictures before we delve into numbers. We've fixed mu_0 at 30, sigma (standard deviation) at 4 and n (sample size) at 16. The function myplot just needs an alternative mean, mu_a, as argument. Run myplot now with an argument of 34 to see what it does.

The distribution represented by H_a moved to the right, so almost all (100%) of the blue curve is to the right of the vertical line, indicating that with mu_a=34, the test is more powerful, i.e., there's a higher probability that it's correct to reject the null hypothesis since it appears false. Now try myplot with an argument of 33.3.

This isn't as powerful as the test with mu_a=34 but it makes a pretty picture. Now try myplot with an argument of 30.

First, power is a function that depends on a specific value of an alternative mean, mu_a, which is any value greater than mu_0, the mean hypothesized by H_0. (Recall that H_a specified mu>30.)

Second, if mu_a is much bigger than mu_0=30 then the power (probability) is bigger than if mu_a is close to 30. As mu_a approaches 30, the mean under H_0, the power approaches alpha.

Just for fun try myplot with an argument of 28. We see that the blue curve has moved to the left of the red, so the area under it, to the right of the line, is less than the 5% under the red curve. This then is even less powerful and contradicts H_a so it's not worth looking at.


We see that the blue curve has moved to the left of the red, so the area under it, to the right of the line, is less than the 5% under the red curve. This then is even less powerful and contradicts H_a so it's not worth looking at.


Here's a picture of the power curves for different sample sizes. Again, this uses code "borrowed" from the slides. The alternative means, the mu_a's, are plotted along the horizontal axis and power along the vertical.

 Now back to numbers. Our test for determining rejection of H_0 involved comparing a test statistic, namely Z=(X'-30)/(sigma/sqrt(n)), against some quantile, say Z_95, which depended on our level size alpha (.05 in this case). H_a proposed that mu > mu_0, so we tested if Z>Z_95. This is equivalent to X' > Z_95 * (sigma/sqrt(n)) + 30, right?

Recall that nifty R function pnorm, which gives us the probability that a value drawn from a normal distribution is greater or less than/equal to a specified quantile argument depending on the flag lower.tail. The function also takes a mean and standard deviation as arguments.

Suppose we call pnorm with the quantile 30 + Z_95 * (sigma/sqrt(n)) and specify mu_a as our mean argument. This would return a probability which we can interpret as POWER. Why?

Recall our picture of two distributions. 30 + Z_95 * (sigma/sqrt(n)) represents the point at which our vertical line falls. It's the point on the null distribution at the (1-alpha) level.

```{r}
pnorm(30+z,mean=30,lower.tail=FALSE)
```

That's not surprising, is it? With the mean set to mu_0 the two distributions, null and alternative, are the same and power=alpha. Now run pnorm now with the quantile 30+z, mean=32, and lower.tail=FALSE.

```{r}
pnorm(30+z,mean=32,lower.tail=FALSE)
```
 See how this is much more powerful? 64% as opposed to 5%. When the sample mean is quite different from (many standard errors greater than) the mean hypothesized by the null hypothesis, the probability of rejecting H_0 when it is false is much higher. That is power!


With this standard deviation=2 (fatter distribution) will power be greater or less than with the standard deviation=1?

```{r}
pnorm(30+z,mean=32,sd = 1, lower.tail=FALSE)
```

```{r}
pnorm(30+z*2,mean=32,sd = 2, lower.tail=FALSE)
```

 See the power drain from 64% to 26% ? Let's review some basic facts about power. We saw before in our pictures that the power of the test depends on mu_a. When H_a specifies that mu > mu_0, then as mu_a grows and exceeds mu_0 increasingly, what happens to power?




If H_a proposed that mu != mu_0 we would calculate the one sided power using alpha / 2 in the direction of mu_a (either less than or greater than mu_0). (This is only approximately right, it excludes the probability of getting a large test statistic in the opposite direction of the truth.

Since power goes up as alpha gets larger would the power of a one-sided test be greater or less than the power of the associated two sided test? GREATER

Suppose H_a says that mu > mu_0. Then power = 1 - beta = Prob ( X' > mu_0 + z_(1-alpha) * sigma/sqrt(n)) assuming that X'~ N(mu_a,sigma^2/n). Which quantities do we know in this statement, given the context of the problem? Let's work through this.

So we know that the quantities mu_0 and alpha are specified by the test designer. In the statement 1 - beta = Prob( X' > mu_0 + z_(1-alpha) * sigma/sqrt(n)) given mu_a > mu_0, mu_0 and alpha are specified, and X' depends on the data. The other four quantities, (beta, sigma, n, and mu_a), are all unknown.

It should be obvious that specifying any three of these unknowns will allow us to solve for the missing fourth. Usually, you only try to solve for power (1-beta) or the sample size n.

An interesting point is that power doesn't need mu_a, sigma and n individually.  Instead only sqrt(n)*(mu_a - mu_0) /sigma is needed. The quantity (mu_a - mu_0) / sigma is called the EFFECT SIZE. This is the difference in the means in standard deviation units. It is unit free so it can be interpreted in different settings.


We'll work through some examples of this now. However, instead of assuming that we're working with normal distributions let's work with t distributions. Remember, they're pretty close to normal with large enough sample sizes.

 Power is still a probability, namely P( (X' - mu_0)/(S /sqrt(n)) > t_(1-alpha, n-1) given H_a that mu > mu_a ). Notice we use the t quantile instead of the z. Also, since the proposed distribution is not centered at mu_0, we have to use the non-central t distribution.

R comes to the rescue again with the function power.t.test. We can omit one of the arguments and the function solves for it. Let's first use it to solve for power.

We'll run it three times with the same values for n (16) and alpha (.05) but different delta and standard deviation values. We'll show that if delta (difference in means) divided by the standard deviation is the same, the power returned will also be the same. In other words, the effect size is constant for all three of our tests.

We'll specify a positive delta; this tells power.t.test that H_a proposes that mu > mu_0 and so we'll need a one-sided test. First run power.t.test(n = 16, delta = 2 / 4, sd=1, type = "one.sample", alt = "one.sided")$power 

```{r}
power.t.test(n = 16, delta = 2 / 4, sd=1, type = "one.sample", alt = "one.sided")$power
```

```{r}
power.t.test(n = 16, delta = 2, sd=4, type = "one.sample", alt = "one.sided")$power
```


```{r}
power.t.test(n = 16, delta = 100, sd=200, type = "one.sample", alt = "one.sided")$power
```

So keeping the effect size (the ratio delta/sd) constant preserved the power. Let's try a similar experiment except now we'll specify a power we want and solve for the sample size n.

```{r}
power.t.test(power = .8, delta = 2 / 4, sd=1, type = "one.sample", alt = "one.sided")$n
```

Now use power.t.test to find delta for a power=.8 and n=26 and sd=1

```{r}
power.t.test(power = .8, n=26, sd=1, type = "one.sample", alt = "one.sided")$delta
```

Now use power.t.test to find delta for a power=.8 and n=27 and sd=1

```{r}
power.t.test(power = .8, n=27, sd=1, type = "one.sample", alt = "one.sided")$delta
```

What do you think will happen if you doubled sd to 2 and ran the same test?

delta will double
```{r}
power.t.test(power = .8, n=27, sd=2, type = "one.sample", alt = "one.sided")$delta
```

The level of a test is specified by what? alpha


What is a Type II error? accepting a false hypothesis

You're a perfectionist designing an experiment and you want both alpha and beta to be small. Can they both be 0 for this single test? No

Suppose H_0 proposes mu = mu_0 and H_a proposes that mu < mu_0. You'll test a series of mu_a with power != alpha. Which of the following is NOT true? mu_a - mu_0=0
because when mu_a = mu_0, power = alpha, which contradicts the question

Suppose H_0 proposes mu = mu_0 and H_a proposes that mu < mu_0. Which of the following is true?
Here mu_a < mu_0 and the smaller mu_a-mu_0 is, the easier it is to discriminate between mu_a and mu_0.

the smaller mu_a-mu_0 the more powerful the test

Which expression represents the size effect?  (mu_a - mu_0) / sigma

 More power is better than less power. true
 
 A larger beta (call it beta_max) is more powerful than a smaller beta. false
 
 The larger the sample size the less powerful the test.
 
 

```{r, eval=FALSE}
z <- qnorm(1 - alpha)

pnorm(mu0 + z*sigma/sqrt(n), mean = mua, sd = sigma/sqrt(n), lower.tail = FALSE)
```


## Example

```{r}
mu0 = 30
mua = 32
sigma = 4
n = 16
alpha = 0.05

z <- qnorm(1 - alpha)
pnorm(mu0 + z*sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n), lower.tail = FALSE)
```

probability of detecting a mean of 32 or larger

```{r}
pnorm(mu0 + z*sigma/sqrt(n), mean = mua, sd = sigma/sqrt(n), lower.tail = FALSE)

```


### manipulate

```{r}
library(manipulate)
library(ggplot2)

mu0 = 30

myplot <- function(sigma, mua, n, alpha){
    
    g = ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
    g = g + stat_function(fun = dnorm, geom = "line",
                          args = list(mean = mu0, sd = sigma/sqrt(n)),
                          size = 2, col = "red")
    
    g = g + stat_function(fun = dnorm, geom = "line",
                          args = list(mean = mua, sd = sigma/sqrt(n)),
                          size = 2, col = "blue")
    xitc = mu0 + qnorm(1 - alpha) * sigma / sqrt(n)
    
    g = g + geom_vline(xintercept = xitc, size = 3)
    
    g
}
```

```{r}
manipulate(
    myplot(sigma, mua, n, alpha),
    sigma = slider(1, 10, step = 1, initial = 4),
    mua = slider(30, 35, step = 1, initial = 32),
    n = slider(1, 50, step = 1, initial = 16),
    alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05)
)
```


### T test Power

delta = mua - mu0

```{r}
power.t.test(n = 16, delta = 2/4, sd = 1, type = "one.sample",
             alt = "one.sided")$power

```


```{r}
 power.t.test(n = 16, delta = 2, sd = 4, type = "one.sample",
             alt = "one.sided")$power

```

```{r}
 power.t.test(n = 16, delta = 100, sd = 200, type = "one.sample",
             alt = "one.sided")$power

```

sample size

```{r}
 power.t.test(power = 0.8, delta = 100, sd = 200, type = "one.sample",
             alt = "one.sided")$n

```
