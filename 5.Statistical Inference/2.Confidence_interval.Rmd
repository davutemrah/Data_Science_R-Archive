---
title: "R Notebook"
output: html_notebook
---

## T distribution

### Confidence Interval

In the Asymptotics lesson we discussed confidence intervals using the Central Limit Theorem (CLT) and
| normal distributions. These needed large sample sizes, and the formula for computing the confidence
| interval was Est +/- qnorm *std error(Est), where Est was some estimated value (such as a sample mean)
| with a standard error. Here qnorm represented what? a specified quantile from a normal distribution

In the Asymptotics lesson we also mentioned the Z statistic Z=(X'-mu)/(sigma/sqrt(n)) which follows a
standard normal distribution. This normalized statistic Z is especially nice because we know its mean and variance. They are what, respectively?

 So the mean and variance of the standardized normal are fixed and known. Now we'll define the t statistic which looks a lot like the Z. It's defined as t=(X'-mu)/(s/sqrt(n)). Like the Z statistic, the t is centered around 0. The only difference between the two is that the population std deviation, sigma, in Z is replaced by the sample standard deviation in the t. So the distribution of the t statistic is independent of the population mean and variance. Instead it depends on the sample size n.
 
 As a result, for t distributions, the formula for computing a confidence interval is similar to what we did in the last lesson. However, instead of a quantile for a normal distribution we use a quantile for a t distribution. So the formula is Est +/- t-quantile *std error(Est). The other distinction, which we mentioned before, is that we'll use the sample standard deviation when we estimate the standard error of Est.

In the formula for the t statistic t=(X'-mu)/(s/sqrt(n)) what expression represents the sample standard deviation? s

These t confidence intervals are very handy, and if you have a choice between these and normal, pick these. We'll see that as datasets get larger, t-intervals look normal. We'll cover the one- and two-group versions which depend on the data you have.

The t distribution, invented by William Gosset in 1908, has thicker tails than the normal. Also, instead of having two parameters, mean and variance, as the normal does, the t distribution has only one - the number of degrees of freedom (df).


As df increases, the t distribution gets more like a standard normal, so it's centered around 0. Also, the t assumes that the underlying data are iid Gaussian so the statistic (X' - mu)/(s/sqrt(n)) has n-1 degrees of freedom.

Quick check. In the formula t=(X' - mu)/(s/sqrt(n)), if we replaced s by sigma the statistic t would be
what asymptotically?. Standart normal


So the t-interval is defined as X' +/- t_(n-1)*s/sqrt(n) where t_(n-1) is the relevant quantile. The t interval assumes that the data are iid normal, though it is robust to this assumption and works well whenever the distribution of the data is roughly symmetric and mound shaped.

Although it's pretty great, the t interval isn't always applicable. For skewed distributions, the spirit of the t interval assumptions (being centered around 0) are violated. There are ways of working around this problem (such as taking **logs** or using a different summary like the **median**).

For highly discrete data, like binary, intervals other than the t are available.



 However, paired observations are often analyzed using the t interval by taking differences between the observations. We'll show you what we mean now.

 We hope you're not tired because we're going to look at some sleep data. This was the data originally analyzed in Gosset's Biometrika paper, which shows the increase in hours for 10 patients on two soporific drugs.



### One sample, paired 

```{r}
rm(list = ls())
data("sleep")

head(sleep)
```


```{r}
group1 = sleep$extra[1:10]
group2 = sleep$extra[11:20]

dif = group2 - group1
mean = mean(dif)
sd = sd(dif)
n = 10
```

```{r}
## CI
mean + c(-1, 1)*qt(.975, n-1)*sd/sqrt(n)
```

This says that with probability .95 the average difference of effects (between the two drugs) for an individual patient is between .7 and 2.46 additional hours of sleep.

We could also just have used the R function t.test with the argument difference to get this result. (You can use the default values for all the other arguments.) As with the other R test functions, this returns a lot of information. Since all we're interested in at the moment is the confidence interval we can pick this off with the construct x$conf.int. Try this now.

```{r}
t.test(dif)$conf.int
```

```{r}
t.test(group2, group1, paired = T)
```

```{r}
t.test(extra ~ I(relevel(group, 2)), paired = T, data = sleep)
```

```{r}
#show 4 different calls to t.test
#display as 4 long array
rbind(
  mean + c(-1, 1) * qt(.975, n-1) * sd / sqrt(n),
  as.vector(t.test(dif)$conf.int),
  as.vector(t.test(group2, group1, paired = TRUE)$conf.int),
  as.vector(t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)$conf.int)
)

```

```{r}
t.test(group2, group1, paired = F)

```
Just as we saw when we ran t.test on our vector, difference! See how the interval excludes 0? This means
the groups when paired have much different averages.



### two sample t-test

We now present methods, using t confidence intervals, for comparing independent groups.

Suppose that we want to compare the mean blood pressure between two groups in a randomized trial. We'll compare those who received the treatment to those who received a placebo. Unlike the sleep study, we cannot use the paired t test because the groups are independent and may have different sample sizes.

So our goal is to find a 95% confidence interval of the difference between two population means. Let's represent this difference as mu_y - mu_x. How do we do this? Recall our formula X' +/- t_(n-1)*s/sqrt(n).

First we need a sample mean, but we have two, X' and Y', one from each group. It makes sense that we'd have to take their difference (Y'-X') as well, since we're looking for a confidence interval that contains the difference mu_y-mu_x. Now we need to specify a t quantile. Suppose the groups have different sizes n_x and n_y.

The only term remaining is the standard error which for the single group is s/sqrt(n). Let's deal with the numerator first. Our interval will assume (for now) a common variance s^2 across the two groups. We'll actually pool variance information from the two groups using a weighted sum. (We'll deal with the more complicated situation later.)

We call the variance estimator we use the pooled variance. The formula for it requires two variance estimators (in the form of the standard deviation), S_x and S_y, one for each group. We multiply each by its respective degrees of freedom and divide the sum by the total number of degrees of freedom. This weights the respective variances; those coming from bigger samples get more weight.

Now recall we're calculating the standard error term which for the single group case was s/sqrt(n). We've got the numerator done, by pooling the sample variances. How do we handle the 1/sqrt(n) portion? We can simply add 1/n_x and 1/n_y and take the square root of the sum. Then we MULTIPLY this by the sample variance to complete the estimate of the standard error.

Now we'll plug in some numbers from the slides based on an example from Rosner's book Fundamentals of Biostatistics, a very good, if heavy, reference book. We want to compare blood pressure from two independent groups.

There are two groups, we assume constant, equal variance

```{r}
library(datasets)
data(ChickWeight)
head(ChickWeight)
```

```{r}
library(reshape2)
library(dplyr)
widechick <- dcast(ChickWeight, 
                   Diet + Chick ~ Time,
                   value.var = "weight")
names(widechick)[-(1:2)] <- paste0("time", names(widechick)[-(1:2)])

widechick <- 
    mutate(widechick, gain = time21 - time0) %>%
    select(-time0:-time21) %>%
    
    filter(Diet %in% c(1, 4))
widechick
```

We dont have background information that they have equal or unequal variance.

Variance is is related to sample size, as well. If there are huge differences in
sample sizes, it is controversial to say that variances are equal.

```{r}
rbind( 
    t.test(gain ~ Diet, paired = F, var.equal = T, data = widechick)$conf,
    t.test(gain ~ Diet, paired = F, var.equal = F, data = widechick)$conf)
```



## two group testing

```{r}
library(UsingR)
data(father.son)

t.test(father.son$sheight - father.son$fheight)
```

```{r}
t.test(father.son$sheight, father.son$fheight, paired = T)

```

## two group testing

there are two different set of chicks, so we use paired false.

we can discuss whether variances are equal or not.

```{r}
t.test(gain ~ Diet, paired = FALSE, 
       var.equal = TRUE, data = widechick)
```


Now let's talk about calculating confidence intervals for two groups which have unequal variances. We
| won't be pooling them as we did before.
In this case the formula for the interval is similar to what we saw before, Y'-X' +/- t_df * SE, where as
| before Y'-X' represents the difference of the sample means. However, the standard error SE and the
| quantile t_df are calculated differently from previous methods. Here SE is the square root of the sum of
| the squared standard errors of the two means, (s_1)^2/n_1 + (s_2)^2/n_2 .

 When the underlying X and Y data are iid normal and the variances are different, the normalized statistic
| we started this lesson with, (X'-mu)/(s/sqrt(n)), doesn't follow a t distribution. However, it can be
| approximated by a t distribution if we set the degrees of freedom appropriately.

The formula for the degrees of freedom is a complicated fraction that no one remembers.  The numerator is
| the SQUARE of the sum of the squared standard errors of the two sample means. Each has the form s^2/n. The
| denominator is the sum of two terms, one for each group. Each term has the same form. It is the standard
| error of the mean raised to the fourth power divided by the sample size-1. More precisely, each term looks
| like (s^4/n^2)/(n-1). We use this df to find the t quantile.




## poisson example

Suppose a hospital is experiencing 10 infections per 100 persons/days. 

Assume that normal, acceptable rate is 0.05.

H0: lambda = 0.05
Ha: lambda > 0.05

```{r}
ppois(9, 5, lower.tail = F)
```

Probablity of having 10 or more infections, when H0 is 0.05 is 0.03. We reject the Null at 5% significance level.

