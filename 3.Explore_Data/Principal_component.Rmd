---
title: "R Notebook"
output: html_notebook
---

### Principal Component Analysis and Singular value decomposition

In this lesson we'll discuss principal component analysis (PCA) and singular value decomposition (SVD), two important and related techniques of dimension reduction. This last entails processes which finding subsets of variables in datasets that contain their essences. PCA and SVD are used in both the exploratory phase and the more formal modelling stage of analysis. We'll focus on the exploratory phase and briefly touch on some of the underlying theory.

lets start with a random generated matrix
```{r}
set.seed(12345)
par(mar = rep(0.2, 4))
dm <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dm)[, nrow(dm):1])
```

# Cluster the data
```{r}
par(mar = rep(0.2, 4))
heatmap(dm)
```
No obvious pattern can be seen above. We can see that even with the clustering that heatmap provides, permuting the rows (observations) and columns (variables) independently, the data still looks random.

## What if we add a pattern?
```{r}
set.seed(678910)
for (i in 1:40) {
    # flip a cooin
    coinflip <- rbinom(1, size =1, prob = 0.5)
    # if a coin is heads add acommon pattern to that row
    if (coinflip) {
        dm[i, ] <- dm[i, ] + rep(c(0, 3), each = 5)
    }
}
par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dm)[, nrow(dm):1])
```
So whether or not a row gets modified by a pattern is determined by a coin flip. Will the added pattern affect every column in the affected row? No.

Here's the image of the altered dataMatrix after the pattern has been added. The pattern is clearly visible in the columns of the matrix. The right half is yellower or hotter, indicating higher values in the matrix.

So in rows affected by the coin flip, the 5 left columns will still have a mean of 0 but the right 5 columns
will have a mean closer to 3.

There is so much noise above.

```{r}
par(mar = rep(0.2, 4))
heatmap(dm)
```


### Patterns in row and column

Again we see the pattern in the columns of the matrix. As shown in the dendrogram at the top of the display,
| these split into 2 clusters, the lower numbered columns (1 through 5) and the higher numbered ones (6 through
| 10). Recall from the code in addPatt.R that for rows selected by the coinflip the last 5 columns had 3 added
| to them. The rows still look random.

| Now consider this picture. On the left is an image similar to the heatmap of dataMatix you just plotted. It is
| an image plot of the output of hclust(), a hierarchical clustering function applied to dataMatrix. Yellow
| indicates "hotter" or higher values than red. This is consistent with the pattern we applied to the data
| (increasing the values for some of the rightmost columns).

The middle display shows the mean of each of the 40 rows (along the x-axis). The rows are shown in the same
| order as the rows of the heat matrix on the left. The rightmost display shows the mean of each of the 10
| columns. Here the column numbers are along the x-axis and their means along the y.

```{r}
hh <- hclust(dist(dm))
dmordered <- dm[hh$order, ]
par(mfrow = c(1, 3))
image(t(dmordered)[, nrow(dmordered):1])

plot(rowMeans(dmordered), 40:1, xlab = "Row Mean", ylab = "Row", pch = 19)
plot(colMeans(dmordered), xlab = "Column", ylab = "Column Mean", pch=19)
```


### Related problems

You have multivariate variables

Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible

If you put all the variables together in one matrix, find the best matrix created with fewer variables

First goal statistical,  then data compression


### PCA/SVD

Now we'll talk a little theory. Suppose you have 1000's of multivariate variables X_1, ... ,X_n. By multivariate we mean that each X_i contains many components, i.e., X_i = (X_{i1}, ... , X_{im}. However, these variables (observations) and their components might be correlated to one another.

As data scientists, we'd like to find a smaller set of multivariate variables that are uncorrelated AND explain as much variance (or variability) of the data as possible. This is a statistical approach.

In other words, we'd like to find the best matrix created with fewer variables (that is, a lower rank matrix) that explains the original data. This is related to data compression.

## SVD

Two related solutions to these problems are PCA which stands for Principal Component Analysis and SVD, Singular Value Decomposition. This latter simply means that we express a matrix X of observations (rows) and variables (columns) as the product of 3 other matrices, i.e., X=UDV^t. This last term (V^t) represents the transpose of the matrix V.

Let X is a matrix with each variable in a columns, each obs is in a row.

Then SVD is a matrix decomposition $$X = UVD^T $$
where the columns of U are orthogonal (left singular), the columns of D are orthogonal (right singular) and D is a diagonal matrix (singular values). Here U and V each have orthogonal (uncorrelated) columns. U's columns are the left singular vectors of X and V's columns are the right singular vectors of X.  D is a diagonal matrix, by which we mean that all of its entries not on the diagonal are 0. The diagonal entries of D are the singular values of X.

```{r}
mat <- matrix(data = c(1, 2, 2, 5, 3, 7), nrow = 2, ncol = 3)

svd(mat)
```

We see that the function returns 3 components, d which holds 2 diagonal elements, u, a 2 by 2 matrix, and v, a 3 by 2 matrix. We stored the diagonal entries in a diagonal matrix for you, diag, and we also stored u and v in the variables matu and matv respectively. Multiply matu by diag by t(matv) to see what you get. (This last expression represents the transpose of matv in R). Recall that in R matrix multiplication requires you to use the operator %*%.

Note that this type of decomposition is NOT unique.

```{r}
matu %*% diag %*% t(matv)
```


#### PCA

Now we'll talk a little about PCA, Principal Component Analysis, "a simple, non-parametric method for extracting relevant information from confusing data sets." We're quoting here from a very nice concise paper on this subject which can be found at http://arxiv.org/pdf/1404.1100.pdf. The paper by Jonathon Shlens of Google Research is called, A Tutorial on Principal Component Analysis.

Basically, PCA is a method to reduce a high-dimensional data set to its essential elements (not lose information) and explain the variability in the data. We won't go into the mathematical details here, (R has a function to perform PCA), but you should know that SVD and PCA are closely related.

First we have to scale mat, our simple example data matrix.  This means that we subtract the column mean from every element and divide the result by the column standard deviation. Of course R has a command, scale, that does this for you. Run svd on scale of mat.

the principal components are equal to the right singular values if you first scale (substract the mean, divide by the standard deviation) the variables.

```{r}
svd(scale(mat))
```

ow run the R program prcomp on scale(mat). This will give you the principal components of mat. See if they look familiar.
```{r}
prcomp(scale(mat))
```

Notice that the principal components of the scaled matrix, shown in the Rotation component of the prcomp output, ARE the columns of V, the right singular values. Thus, PCA of a scaled matrix yields the V matrix (right singular vectors) of the same scaled matrix.

Now that we covered the theory let's return to our bigger matrix of random data into which we had added a fixed pattern for some rows selected by coinflips. The pattern effectively shifted the means of the rows and columns.

## Dimension Reduction

#### Components of the SVD -u and -v

```{r}
svd1 <- svd(scale(dmordered))

par(mfrow = c(1, 3))

image(t(dmordered)[, nrow(dmordered):1])

plot(svd1$u[, 1], 40:1, xlab = "Row", ylab = "First left singular vector", pch =19)
plot(svd1$v[, 1], xlab = "Column", ylab = "First right singular vector", pch = 19)
```
Here we again show the clustered data matrix on the left. Next to it we've plotted the first column of the U matrix associated with the scaled data matrix. This is the first LEFT singular vector and it's associated with the ROW means of the clustered data. You can see the clear separation between the top 24 (around -0.2) row means and the bottom 16 (around 0.2). We don't show them but note that the other columns of U don't show this pattern so clearly.

The rightmost display shows the first column of the V matrix associated with the scaled and clustered data matrix. This is the first RIGHT singular vector and it's associated with the COLUMN means of the clustered data. You can see the clear separation between the left 5 column means (between -0.1 and 0.1) and the right 5 column means (all below -0.4). As with the left singular vectors, the other columns of V don't show this pattern as clearly as this first one does.

So the singular value decomposition automatically picked up these patterns, the differences in the row and column means.

Why were the first columns of both the U and V matrices so special?  Well as it happens, the D matrix of the SVD explains this phenomenon. It is an aspect of SVD called variance explained. Recall that D is the diagonal matrix sandwiched in between U and V^t in the SVD representation of the data matrix. The diagonal entries of D are like weights for the U and V columns accounting for the variation in the data. They're given in decreasing order from highest to lowest. Look at these diagonal entries now. Recall that they're stored in svd1$d.

```{r}
svd1$d
```

## Components of the SVD - Variance expleained

Here's a display of these values (on the left). The first one (12.46) is significantly bigger than the others. Since we don't have any units specified, to the right we've plotted the proportion of the variance each entry represents. We see that the first entry accounts for about 40% of the variance in the data. This explains why the first columns of the U and V matrices respectively showed the distinctive patterns in the row and column means so clearly.

On the right graph, you can see how much of the variance is explained by each column

```{r}
par(mfrow = c(1, 2))

plot(svd1$d, xlab = "Column", ylab = "Singular Value", pch = 19)

plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained",
     pch = 19)
```

### Relationship to principal components

Here's a picture showing the relationship between PCA and SVD for that bigger matrix.  We've plotted 10 points (5 are squished together in the bottom left corner). The x-coordinates are the elements of the first principal component (output from prcomp), and the y-coordinates are the elements of the first column of V, the first right singular vector (gotten from running svd). We see that the points all lie on the 45 degree line represented by the equation y=x.  So the first column of V IS the first principal component of our bigger data matrix.

 To prove we're not making this up, we've run svd on dataMatrix and stored the result in the object svd1. This has 3 components, d, u and v. look at the first column of V now. It can be viewed by using the svd1$v[,1] notation.

Here we can observe that PCA and SVD are pretty much same

```{r}
svd1$v[,1]
```

```{r}
svd1 <- svd(scale(dmordered))
pca1 <- prcomp(dmordered, scale = TRUE)
plot(pca1$rotation[, 1], svd1$v[, 1], pch = 19, xlab="Principal Component 1",
     ylab = "Right Singular Vector 1")
abline(c(0, 1))
```
See how these values correspond to those plotted? Five of the entries are slightly to the left of the point (-0.4,-0.4), two more are negative (to the left of (0,0)), and three are positive (to the right of (0,0)).

### Components of the SVD - variance explained

Now we'll show you another simple example of how SVD explains variance. We've created a 40 by 10 matrix, constantMatrix. Use the R command head with constantMatrix as its argument to see the top rows.
The rest of the rows look just like these. You can see that the left 5 columns are all 0's and the right 5 columns are all 1's. We've run svd with constantMatrix as its argument for you and stored the result in svd2. Look at the diagonal component, d, of svd2 now.

```{r}
constantmatrix <- dmordered*0

for(i in 1:dim(dmordered)[1]){ 
    constantmatrix[i, ] <- rep(c(0, 1), each = 5)
}

svd2 <- svd(constantmatrix)

svd2$d
```

Which index holds the largest entry of the svd2$d? 1 

 So the first entry by far dominates the others. Here the picture on the left shows the heat map of constantMatrix. You can see how the left columns differ from the right ones. The middle plot shows the values of the singular values of the matrix, i.e., the diagonal elements which are the entries of svd2$d. Nine of these are 0 and the first is a little above 14. The third plot shows the proportion of the total each diagonal element represents.

According to the plot, what percentage of the total variation does the first diagonal element account for? 100%

```{r}
par(mfrow = c(1, 3))
image(t(constantmatrix)[, nrow(constantmatrix):1])
plot(svd2$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd2$d^2/sum(svd2$d^2), xlab = "Column", ylab = "Prop. of variance explained",
     pch = 19)
```

So what does this mean? Basically that the data is one-dimensional. Only 1 piece of information, namely which column an entry is in, determines its value.

## What if we add a second pattern ?

Now let's return to our random 40 by 10 dataMatrix and consider a slightly more complicated example in which we add 2 patterns to it. Again we'll choose which rows to tweak using coinflips. Specifically, for each of the 40 rows we'll flip 2 coins. If the first coinflip is heads, we'll add 5 to each entry in the right 5 columns of that row, and if the second coinflip is heads, we'll add 5 to just the even columns of that row.

```{r}
set.seed(678910)
for (i in 1:40) {
    # flip a coin
    coinflip1 <- rbinom(n = 1, size = 1, prob = 0.5)
    coinflip2 <- rbinom(n = 1, size = 1, prob = 0.5)
    # if coin is heads, add a common pattern to that row
    
    if(coinflip1) {
        dm[i, ] <- dm[i, ] + rep(c(0, 5), each = 5)
    }
    
    if(coinflip2) {
        dm[i, ] <- dm[i, ] + rep(c(0, 5), 5)
    }
}

hh <- hclust(dist(dm))
dmordered <- dm[hh$order, ]
```

### Singular value composition - true patterns
So here's the image of the scaled data matrix on the left. We can see both patterns, the clear difference between the left 5 and right 5 columns, but also, slightly less visible, the alternating pattern of the columns. The other plots show the true patterns that were added into the affected rows. The middle plot shows the true difference between the left and right columns, while the rightmost plot shows the true difference between the odd numbered and even-numbered columns.

```{r}
svd2 <- svd(scale(dmordered))
par(mfrow = c(1, 3))

image(t(dmordered)[, nrow(dmordered):1])
plot(rep(c(0,1), each=5), pch = 19, xlab = "Column",
     ylab = "Pattern 1")
plot(rep(c(0, 1), 5), pch = 19, xlab = "Column",
     ylab = "Pattern 2")
```
The question is, "Can our analysis detect these patterns just from the data?" Let's see what SVD shows. Since we're interested in patterns on columns we'll look at the first two right singular vectors (columns of V) to see if they show any evidence of the patterns.

### v and patterns of variance in rows

Here we see the 2 right singular vectors plotted next to the image of the data matrix. The middle plot shows the first column of V and the rightmost plot the second. The middle plot does show that the last 5 columns have higher entries than the first 5. This picks up, or at least alludes to, the first pattern we added in which affected the last 5 columns of the matrix. The rightmost plot, showing the second column of V, looks more random. However, closer inspection shows that the entries alternate or bounce up and down as you move from left to right. This hints at the second pattern we added in which affected only even columns of selected rows.

```{r}
svd2 <- svd(scale(dmordered))
par(mfrow = c(1, 3))

image(t(dmordered)[, nrow(dmordered):1])
plot(svd2$v[, 1], pch = 19, xlab = "Column", ylab = "First right singular vector")
plot(svd2$v[, 2], pch = 19, xlab = "column", ylab = "Second right singular vector")
```
To see this more closely, look at the first 2 columns of the v component. We stored the SVD output in the svd object svd2.
```{r}
svd2$v[,1:2]
```
Seeing the 2 columns side by side, we see that the values in both columns alternately increase and decrease. However, we knew to look for this pattern, so chances are, you might not have noticed this pattern if you hadn't known if was there. This example is meant to show you that it's hard to see patterns, even straightforward ones.

## d and variance explained

Now look at the entries of the diagonal matrix d resulting from the svd. Recall that we stored this output for you in the svd object svd2.
```{r}
svd2$d
```

We see that the first element, 14.55, dominates the others. Here's the plot of these diagonal elements of d. The left shows the numerical entries and the right show the percentage of variance each entry explains.

```{r}
svd1 <- svd(scale(dmordered))

par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Percent of variance explained", pch = 19)

```
So the first element which showed the difference between the left and right halves of the matrix accounts for roughly 50% of the variation in the matrix, and the second element which picked up the alternating pattern accounts for 18% of the variance. The remaining elements account for smaller percentages of the variation. This indicates that the first pattern is much stronger than the second. Also the two patterns confound each other so they're harder to separate and see clearly. This is what often happens with real data.

Now you're probably convinced that SVD and PCA are pretty cool and useful as tools for analysis, but one problem with them that you should be aware of, is that they cannot deal with MISSING data. Neither of them will work if any data in the matrix is missing. (You'll get error messages from R in red if you try.) Missing data is not unusual, so luckily we have ways to work around this problem. One we'll just mention is called imputing the data.


### Handle Missing values in data

This uses the k nearest neighbors to calculate a values to use in place of the missing data. You may want to specify an integer k which indicates how many neighbors you want to average to create this replacement value. The bioconductor package (http://bioconductor.org) has an impute package which you can use to fill in missing data. One specific function in it is impute.knn.

Here we compare imputed data and normal data

Looks like almost same!! But, this data is already randomly generated.


```{r}
library(impute)
dm2 <- dmordered
# randomly insert missing values
dm2[sample(1:100, size = 40, replace = FALSE)] <- NA

dm2 <- impute.knn(dm2)$data

svd1 <- svd(scale(dm2))  ## does not work if missing values exist
svd2 <- svd(scale(dm2))

par(mfrow = c(1, 2))
plot(svd1$v[, 1], pch = 19)
plot(svd2$v[, 1], pch = 19)
```

We'll move on now to a final example of the power of singular value decomposition and principal component analysis and how they work as a data compression technique.

## Face example

Consider this low resolution image file showing a face. We'll use SVD and see how the first several components contain most of the information in the file so that storing a huge matrix might not be necessary.

Let's see how big The image data  is.

```{r}
load("data/face.rda")
dim(faceData)
```


```{r}
image(t(faceData)[, nrow(faceData):1])
```

Lower resolution data

So it's not that big of a file but we want to show you how to use what you learned in this lesson. We've done the SVD and stored it in the object svd1 for you. Here's the plot of the variance explained.

It seems that first 5 singular vectors can explain much of the data

```{r}
svd1 <- svd(scale(faceData))

plot(svd1$d^2/sum(svd1$d^2), pch = 19, xlab = "Singular vector",
     ylab = "Variance explained")

```

According to the plot what percentage of the variance is explained by the first singular value? 40%

So 40% of the variation in the data matrix is explained by the first component, 22% by the second, and so forth. It looks like most of the variation is contained in the first 10 components. How can we check this out? Can we try to create an approximate image using only a few components?

Recall that the data matrix X is the product of 3 matrices, that is X=UDV^t. These are precisely what you get when you run svd on the matrix X. Suppose we create the product of pieces of these, say the first columns of U and V and the first element of D. The first column of U can be interpreted as a 32 by 1 matrix (recall that faceData was a 32 by 32 matrix), so we can multiply it by the first element of D, a 1 by 1 matrix, and get a 32 by 1 matrix result. We can multiply that by the transpose of the first column of V, which is the first principal component. (We have to use the transpose of V's column to make it a 1 by 32 matrix in order to do the matrix multiplication properly.)

#### Face example - Create approximations 

Alas, that is how we do it in theory, but in R using only one element of d means it's a constant. So we have to do the matrix multiplication with the %*%  operator and the multiplication by the constant (svd1$d[1]) with the regular multiplication operator *.

Try this now and put the result in the variable a1. Recall that svd1$u, svd1$d, and svd1$v contain all the information you need. NOTE that because of the peculiarities of R's casting, if you do the scalar multiplication with the * operator first (before the matrix multiplication with the %*% operator) you MUST enclose the 2 arguments (svd1$u[,1] and svd1$d[1]) in parentheses.

```{r}
svd1 <- svd(scale(faceData))
## Note that %*% matrix multiplication

## Here svd1$d[1] is a constant
approx1 <- (svd1$u[, 1] * svd1$d[1]) %*% t(svd1$v[, 1]) 
```

(reducing dimension by selecting representative data)

```{r}
# in these examples we need to make the diagonal matrix out of d
approx5 <- svd1$u[, 1:5]  %*% diag(svd1$d[1:5])  %*% t(svd1$v[, 1:5])
approx10 <- svd1$u[, 1:10]  %*% diag(svd1$d[1:10])  %*% t(svd1$v[, 1:10])
```

```{r}
par(mfrow = c(1, 4))
image(t(approx1)[, nrow(approx1):1], main = "(a)")
image(t(approx5)[, nrow(approx5):1], main = "(b)")
image(t(approx10)[, nrow(approx10):1], main = "(c)")
image(t(faceData)[, nrow(faceData):1], main = "(d)")

```

## Summary and notes

We'll close now with a few comments. First, when reducing dimensions you have to pay attention to the scales on which different variables are measured and make sure that all your data is in consistent units. In other words, scales of your data matter. Second, principal components and singular values may mix real patterns, as we saw in our simple 2-pattern example, so finding and separating out the real patterns require some detective work. Let's do a quick review now.

* Scale matters: mix level of scaled data is harder
* PCA computationally intensive
* factor analysis, independent components analysis share same idea that you want to find the kind of lower dimensional representation that explains most of the variation in the data that you see.

A matrix X has the singular value decomposition UDV^t. The principal components of X are ?
Recall the simple example where we ran prcomp and svd on the same scaled matrix and saw that the columns of V matched the rotations of the prcomp output. Columns of v.

A matrix X has the singular value decomposition UDV^t. The singular values of X are found where? the diagonal elements of D. D gives the singular values of a matrix in decreasing order of weight.






